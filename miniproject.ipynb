{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH9UoY_qqj2A"
   },
   "source": [
    "<font size=\"5\">Mini Project on ResNet</font>\n",
    "\n",
    "In this mini project, our task is to train a deep convolutional neural network to perform image classification.\n",
    "\n",
    "We will train ResNet using the CIFAR10 dataset, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The classes are: {airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLz9lM5Iqj2C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch.utils.data as data\n",
    "from torchsummary import summary\n",
    "\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx7UinV5qj2E"
   },
   "source": [
    "<font size=\"5\">Loading and Preparing the Data</font>\n",
    "\n",
    "Our dataset is made up of color images but three color channels (red, green and blue). To normalize our data we need to calculate the means and standard deviations for each of the color channels independently, and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-CUQI5Pqj2E",
    "outputId": "bf9aa328-19ac-4327-d37e-63df17387d6c"
   },
   "outputs": [],
   "source": [
    "ROOT = '.data'\n",
    "train_data = datasets.CIFAR10(root = ROOT, \n",
    "                              train = True, \n",
    "                              download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKQtu8krQMcz"
   },
   "source": [
    "Compute means and standard deviations along the R,G,B channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aN2bh7mQWdM"
   },
   "outputs": [],
   "source": [
    "means = train_data.data.mean(axis = (0,1,2)) / 255\n",
    "stds = train_data.data.std(axis = (0,1,2)) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-0357sBqj2E"
   },
   "source": [
    "Next, we will do data augmentation. For each training image we will randomly flip it horizontally, padding it by 4 and randomly crop part of it (32*32). Finally we will normalize each color channel using the means/stds we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MU_J_f1qj2E"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = means, std = stds)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = means, std = stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bAPYPF9qj2F"
   },
   "source": [
    "Next, we'll load the dataset along with the transforms defined above.\n",
    "\n",
    "We will also create a validation set with 10% of the training samples. The validation set will be used to monitor loss along different epochs, and we will pick the model along the optimization path that performed the best, and report final test accuracy numbers using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TYaxxaJqj2F",
    "outputId": "fee19301-6450-4dba-ae1c-b236597725de"
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "train_data = datasets.CIFAR10(ROOT, \n",
    "                              train = True, \n",
    "                              download = True, \n",
    "                              transform = train_transforms)\n",
    "\n",
    "test_data = datasets.CIFAR10(ROOT, \n",
    "                             train = False, \n",
    "                             download = True, \n",
    "                             transform = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7x0FykHqj2F"
   },
   "source": [
    " Create data loaders for train_data, valid_data, test_data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "086jVqO0qj2G"
   },
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 128\n",
    "BATCH_SIZE = 256\n",
    "#BATCH_SIZE = 512\n",
    "\n",
    "train_iterator = data.DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "valid_iterator = data.DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=False)\n",
    "test_iterator = data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMHe3XeBqj2G"
   },
   "source": [
    "<font size=\"5\">Defining the Model</font>\n",
    "\n",
    "Next up is defining the model.\n",
    "ResNet will have the following architecture:\n",
    "ReLU(S(x) + F(x))\n",
    "where S(x) refers to the skipped connection and F(x) is a block that implements conv -> BN -> relu -> conv -> BN\n",
    "\n",
    "Also, by reading this paper https://arxiv.org/abs/1603.05027\n",
    "we have a new design F(x) implements -> BN -> relu -> conv -> BN -> relu -> conv (then do addition without relu)\n",
    "\n",
    "Our code references https://github.com/FrancescoSaverioZuppichini/ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z30mibwLqj2G"
   },
   "source": [
    "<font size=\"4\">Basic Block</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQia13-3qj2G"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()  \n",
    "\n",
    "        ## design 1\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        ## design 2\n",
    "        # self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        # self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "        #                        stride=stride, padding=1, bias=False)\n",
    "        # self.bn2 = nn.BatchNorm2d(planes)\n",
    "        # self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "        #                        stride=1, padding=1, bias=False)   \n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## design 1\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        ## design 2\n",
    "        # out = F.relu(self.bn1(x))\n",
    "        # out = F.relu(self.bn2(self.conv1(x)))\n",
    "        # out = self.conv2(out)\n",
    "        # out += self.shortcut(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUgiMhoTqj2H"
   },
   "source": [
    "<font size=\"5\">ResNet</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7S_YSKnqj2H"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(128*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPmgkel3qj2I"
   },
   "source": [
    "We'll create an instance of our model with the desired amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr40Fd4aqj2I"
   },
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, [3, 4, 23, 3]) \n",
    "#[3, 4, 23, 3] resnet-101\n",
    "#[3, 8, 36, 3] resnet-152\n",
    "#[3, 24, 36, 3] resnet-200\n",
    "summary(model.cuda(), (3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6vlfZmHqj2I"
   },
   "source": [
    "<font size=\"5\">Train the Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VbTfNKqj2I"
   },
   "source": [
    "We then define the loss function we want to use, the device we'll use and place our model and criterion on to our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcubTQWaqj2J"
   },
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "##SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "##ADAM\n",
    "#optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etS6T9tWqj2J"
   },
   "source": [
    "define a function to calculate accuracy, train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGpTU0XHqj2J"
   },
   "outputs": [],
   "source": [
    "train_record = {}\n",
    "valid_record = {}\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    train_record[epoch]=[]\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_iterator):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100.*correct/total\n",
    "        train_record[epoch]+=[acc]\n",
    "        print('train b_id: ',batch_idx,' Loss: ', train_loss/(batch_idx+1),', Acc:', acc,' ;', correct,'/', total)  \n",
    "\n",
    "def valid(epoch):\n",
    "    valid_record[epoch]=[]\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valid_iterator):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/total\n",
    "            valid_record[epoch]+=[acc]\n",
    "            print('valid b_id: ',batch_idx,' Loss: ', valid_loss/(batch_idx+1),', Acc:', acc,' ;', correct,'/', total)\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "        best_acc = acc\n",
    "        \n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(iterator):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return test_loss/(batch_idx+1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wseBNVl9qj2J"
   },
   "source": [
    "Then, finally, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqZ0mJmBqj2J",
    "outputId": "784f390e-bfed-42c8-f4d4-c976e7edd71d"
   },
   "outputs": [],
   "source": [
    "#EPOCHS = 25\n",
    "#EPOCHS = 35\n",
    "#EPOCHS = 45\n",
    "EPOCHS = 85\n",
    "\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+EPOCHS):\n",
    "    train(epoch)\n",
    "    valid(epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "# save data\n",
    "train_record_file = open(\"train_record.json\", \"w\")  \n",
    "json.dump(train_record, train_record_file)  \n",
    "train_record_file.close()  \n",
    "\n",
    "test_record_file = open(\"test_record.json\", \"w\")  \n",
    "json.dump(valid_record, test_record_file)  \n",
    "test_record_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8ZrKpgdRh3M"
   },
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu20dvO-Roe8"
   },
   "outputs": [],
   "source": [
    "best_model = ResNet(BasicBlock, [3, 4, 23, 3])\n",
    "best_model.load_state_dict(torch.load('best-model.pt'))\n",
    "best_model = best_model.to(device)\n",
    "test_loss, test_acc = evaluate(best_model, test_iterator, criterion, device)\n",
    "print('Loss: ', test_loss,', Acc:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
